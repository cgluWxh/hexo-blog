---
title: MAT 2040
date: 2025-03-26 20:41:00
tags: [龙大, 学业]
---

### 你或许会更想去 [Notekit](https://notes.bilibiili.com/share?shareId=505322-iYoC-ilCqt16) 看这份笔记

#### ​MAT 2040​ Course Content

- 进考场之前看什么

	- Det = 0 说明是 singular 也就是 not invertible {% raw %} $$Ax=0$$ {% endraw %} has infinite many solution 行列向量是线性相关的
		Det != 0 说明是 nonsingular 也就是 invertible {% raw %} $$ Ax = 0 $$ {% endraw %}  has only a trivial solution 说明是满秩的(但非方矩阵满秩不一定推出 invertible) 行列向量是线性无关的

	- ((Property 16.6 (Determinants of Three Elementary Matrices)
		第一类：行交换 {% raw %} $$det=-1$$ {% endraw %} 
		第二类：行标量乘 {% raw %} $$det=\alpha$$ {% endraw %} 
		第三类：一行加另一行标量乘 {% raw %} $$det=1$$ {% endraw %} ))

	- ((Definition 16.13 Adjoint Matrix
		在原矩阵 {% raw %} $$a_{ij}$$ {% endraw %}  位置放 Cofactor {% raw %} $$A_{ij}=(-1)^{i+j}det(M_{ij})$$ {% endraw %}  再 Transpose 就得到了 Adjoint Matrix {% raw %} $$A\ adj(A)=det(A)I_n$$ {% endraw %}  {% raw %} $$If\ det(A)\neq 0, A^{-1}=\frac{1}{det(A)}adj(A)$$ {% endraw %}  -> method to find {% raw %} $$A^{-1}$$ {% endraw %} ))

	- ((Linear transformation
		线性变换是一种保持向量加法和标量乘法的映射. 这意味着：
		向量的加法在变换前后保持一致.
		标量乘法在变换前后保持一致.))

	- ((Example 17.14
		二维平面起点为原点的逆时针旋转： {% raw %} $$A=\left[\begin{matrix}\cos\theta & -\sin\theta\\ \sin\theta & \cos\theta\end{matrix}\right]\\
		L\left(\left[\begin{matrix}x\\ y\end{matrix}\right]\right)=A\left[\begin{matrix}x\\ y\end{matrix}\right]=\left[\begin{matrix}x\cos\theta-y\sin\theta\\ x\sin\theta+y\cos\theta\end{matrix}\right]$$ {% endraw %} 二维平面起点为原点的顺时针旋转 {% raw %} $$B=\left[\begin{matrix}\cos\theta & \sin\theta\\ -\sin\theta & \cos\theta\end{matrix}\right]\\
		L\left(\left[\begin{matrix}x\\ y\end{matrix}\right]\right)=B\left[\begin{matrix}x\\ y\end{matrix}\right]=\left[\begin{matrix}xcos\theta+ysin\theta\\ -xsin\theta+ycos\theta\end{matrix}\right]$$ {% endraw %} ))

	- ((Theorem 18.1 (Matrix Representation for General Vector Spaces) {% raw %} $$L: V\to W\,\,[L({\bf u})]_w=A[{\bf u}]_v,\forall{\bf u} \in V$$ {% endraw %} ，转换矩阵 {% raw %} $$A$$ {% endraw %} 每列为 {% raw %} $$V$$ {% endraw %} 基底提前经 {% raw %} $$L$$ {% endraw %} 变换后转到 {% raw %} $$W$$ {% endraw %} ))

	- ((Theorem 19.17 (Fundamental Subspaces Theorem) {% raw %} $$Null(A)=Col(A^T)^\perp=Row(A)^\perp$$ {% endraw %}  {% raw %} $$Null(A^T)=Col(A)^\perp=Row(A^T)^\perp$$ {% endraw %} ))

	-  {% raw %} $$dim\ S+dim\ S^\perp = n$$ {% endraw %} 

	-  {% raw %} $$A^T(\mathbf{b}-A\hat{\mathbf{x}})=0\\
		A^TA\hat{\mathbf{x}}=A^T\mathbf{b}$$ {% endraw %} (Normal Equation)

	- 最小二乘估计![](https://notes.bilibiili.com/v2/data/images/505322/1b17a1d8ae0bd2e23e115b65686e349e.png)

	- ((Definition 21.15 (Orthogonal Matrix) {% raw %} $$Q\in \R^{n\times n}$$ {% endraw %} ，若 {% raw %} $$Q$$ {% endraw %}  的列向量是 {% raw %} $$\R^n$$ {% endraw %}  中的一个OrthonormalSet，则 {% raw %} $$Q$$ {% endraw %}  是OrthogonalMatrix)) {% raw %} $$Q^{-1}=Q^T$$ {% endraw %} 

	-  {% raw %} $$A=QR$$ {% endraw %}  要求 {% raw %} $$\text{rank}(A)=n$$ {% endraw %}  {% raw %} $$Q$$ {% endraw %}  为 Orthogonal Matrix，是经Gram-Schmidt 得到的， {% raw %} $$R=Q^{-1}A=Q^TA$$ {% endraw %} 

	- 对于一个可QR分解的矩阵，算Normal Equation有简化方法((Theorem: {% raw %} $$A^TA\hat{x}=A^T{\bf b}$$ {% endraw %}  {% raw %} $$R^TR\hat{x}=R^TQ^Tb$$ {% endraw %} Finally: {% raw %} $$R\hat{x}=Q^T\mathbf{b}$$ {% endraw %}  {% raw %} $$\hat{x}=R^{-1}Q^T\mathbf{b}$$ {% endraw %} ))

	- Characteristic Equation {% raw %} $$p_A(\lambda)=\text{det}(A-\lambda I)=0$$ {% endraw %} 

	-  {% raw %} $$\text{Null}(A-\lambda I)$$ {% endraw %} is called the eigenspace corresponding to {% raw %} $$\lambda$$ {% endraw %} 

	-  {% raw %} $$\displaystyle\text{det}(A)=\prod_{i=1}^n \lambda_i$$ {% endraw %} 行列式=特征值积 {% raw %} $$\displaystyle\sum_{i=1}^n a_{ii}=\sum_{i=1}^n\lambda_i=tr(A)$$ {% endraw %} 迹=特征值和

	- (( {% raw %} $$A$$ {% endraw %} is nonsingular and {% raw %} $$\lambda$$ {% endraw %}  is the eigenvalue of {% raw %} $$A$$ {% endraw %} , Then {% raw %} $$\iff \lambda^{-1}$$ {% endraw %}  is the eigenvalue of {% raw %} $$A^{-1}$$ {% endraw %} ))

	- 相似矩阵有相同特征值

	- 不同eigenvalue对应的eigenvector线性独立

	- A {% raw %} $$n\times n$$ {% endraw %}  matrix {% raw %} $$A$$ {% endraw %}  is diagonalizable iff {% raw %} $$A$$ {% endraw %}  has {% raw %} $$n$$ {% endraw %}  linearly independent eigenvectors

	- 对称矩阵的一定有n个不等的eigenvalues，所对应向量相互垂直

	- Diagonalize: {% raw %} $$D=Q^{-1}AQ$$ {% endraw %} ，求 {% raw %} $$D$$ {% endraw %} 的方法： {% raw %} $$\left[\begin{matrix}\lambda_1&0&0\\0&\lambda_2&0\\0&0&\lambda_3\end{matrix}\right]$$ {% endraw %} ，求 {% raw %} $$Q$$ {% endraw %} 的方法：把 eigenvalue 对应的 eigenvector 作为列向量组成矩阵；对于对称矩阵有 {% raw %} $$D=Q^TAQ=\lambda_1\mathbf{q}_1\mathbf{q}_1^T+\cdots+\lambda_n\mathbf{q}_n\mathbf{q}_n^T$$ {% endraw %} 

	-  {% raw %} $$f({\bf x})={\bf x}^TA{\bf x}$$ {% endraw %}  根据是否与 {% raw %} $${\bf x}$$ {% endraw %} 的取值无关而>(=) <(=) 0 有 positive (semi)definite & negative (semi)definite & indefinite

	- Postive Definite 等价 Eigenvalues全正 等价 LeadingPrincipalMinors全正 等价 {% raw %} $$A=C^2$$ {% endraw %}  等价 {% raw %} $$A=LL^T$$ {% endraw %} (Lower Triangular 对角>0) 等价 {% raw %} $$A=LDL^T$$ {% endraw %} 

	- ((Gram-Schmidt Process 格拉姆-施密特正交化))

- Slide 1 - Linear Systems and Matrices I

	- Notations {% raw %} $$\vec{u} \to Row\ Vector\\
		\underline{u}\to Column\ Vector$$ {% endraw %} 

	- Special Matrix {% raw %} $$O \to Zero\ Matrix \\
		I\to Identity\ Matrix\\
		\vec{0} \to Zero\ Vector$$ {% endraw %} 

	- Definition 1.17 (Row-Equivalent Matrices) Two matrices are said to be row equivalent if one can obtained from the other by a sequence of elementary row operations.

	- Theorem 1.18 (Equivalent Linear Systems) Consider two linear systems. Row operations for the augmented matrix preserve the solution set of the linear system.

	- Forward Elimination -> the upper triangular matrix (至少会得到一个 Row Echelon Form)
		Backward substitution -> Solution

- Slide 2 - Linear Systems and Matrices II Slide02.pdf

	- Gaussian Elimination + Back Substitution

	- Definition 2.1 (Row Echelon Form)
		前导零更多的行靠下 & 全0的行在最下

	- Reduced row echelon form (RREF)
		是 Row Echelon Form & 每行前导为1 & 前导1为整列唯一非0

	-  {% raw %} $$\#\ non\ zero\ rows=\#\ of\ pivot\ columns=\#\ of\ leading\ 1's$$ {% endraw %} 

	- -> Upper triangular form 一定是 Row echelon form，但是 Row echelon form 不一定是 Upper triangular form

- Slide 3 - Linear Systems and Matrices III Slide03(1).pdf

	- Consistence {% raw %} $$Consistent\to 有解\\
		Inconsistent\to 无解$$ {% endraw %} 

	- Consistent 等价 （Augmented Matrix 的最右列不是一个 Pivot Column）

	- 当 Pivot 数与未知数的数量相等时有唯一解. 否则有无数解，若有 {% raw %} $$r$$ {% endraw %}  个 Pivot，可有 {% raw %} $$n-r$$ {% endraw %}  个 Free/Independent Variables

	- 一个 Linear Equations System 有唯一解、无数解或无解

	- 当我们有一行 {% raw %} $$\begin{matrix}[ 1 & 0 & 3 & -2 & | & 4 ]\end{matrix}$$ {% endraw %} ，我们写 {% raw %} $$x_1=4-3x_3+2x_4$$ {% endraw %} 

	- Homogeneous System即 {% raw %} $$A\boldsymbol{x} = \boldsymbol{0}$$ {% endraw %} 

		- 总是有解因为 {% raw %} $$\boldsymbol{x}=\boldsymbol{0}$$ {% endraw %}  总是一个解 这个解叫 Trivial Solution

		- Underdetermined homogeneous systems 有无数个解 {% raw %} $$\#\ of\ pivot\ columns=\#\ of\ nonzero\ rows\leq m \lt n$$ {% endraw %} 

	- Underdetermined consistent systems 有无数个解 {% raw %} $$\#\ of\ pivot\ columns=\#\ of\ nonzero\ rows\leq m\lt n$$ {% endraw %} 

- Slide 4 - Matrices Algebra I Slide04(1).pdf

	- Set of Matrices {% raw %} $$\R^{m\times n}\ \Complex^{m\times n}$$ {% endraw %} 

	- Set of Column Vectors {% raw %} $$\R^n=\R^{n\times 1}\ \Complex^n=\Complex^{n\times 1}$$ {% endraw %} 

	- 矩阵相等 每一项相等

	- 矩阵相加 row=row col=col

	- 标量乘 用标量乘每一项

	- Zero Matrix 每一项都是 {% raw %} $$0\ O_{m\times n}$$ {% endraw %} 

	- 列向量 行向量

	- 矩阵加和标量乘均满足：交换率 结合率 分配率（标量乘）

	- 矩阵乘向量
		用每一行向量乘以该向量，得新行向量
		|| alinear combination of column vectors {% raw %} $$a_1,a_2,\cdots,a_n$$ {% endraw %}  with weights {% raw %} $$u_1,\cdots,u_n$$ {% endraw %} 

		- 满足分配律，满足 {% raw %} $$A(\alpha \boldsymbol{x})=(\alpha A)\boldsymbol{x}=\alpha(A \boldsymbol{x})$$ {% endraw %} 

	- ![](https://notes.bilibiili.com/v2/data/images/505322/0122429854545f810a19291603ceb599.png)

	- Equivalent Condition for a Consistent Linear SystemThe linear system {% raw %} $$A\boldsymbol{x} = \boldsymbol{b}$$ {% endraw %}  is consistent if and only if {% raw %} $$\boldsymbol{b}$$ {% endraw %}  is a linear combination of the column vectors of {% raw %} $$A$$ {% endraw %} .

	- Matrix Product {% raw %} $$A \times B = [A\boldsymbol{b}_1,\cdots,A\boldsymbol{b}_r] =C (A \in \R^{m\times n}, B\in \R^{n\times R})\\
		c_{ij} = \sum^{n}_{k=1} a_{ik}b_{kj}=\vec{a}_i\boldsymbol{b}_j$$ {% endraw %} 用 A 的行乘 B 的列，所以 A 每行元素与 B 每列元素相等，也就是说 A 的列 = B 的行

		- 注意⚠️ 不满足交换，也不可等式两边约： {% raw %} $$AB=AC$$ {% endraw %}  不说明 {% raw %} $$B=C$$ {% endraw %} 

		- 满足分配律，同时满足 {% raw %} $$\alpha(AB)=(\alpha A)B=A(\alpha B)$$ {% endraw %} 

- Slide 5 - Matrices Algebra II Slide05.pdf

	-  {% raw %} $$AB=O$$ {% endraw %}  does not imply that {% raw %} $$A=O$$ {% endraw %}  or {% raw %} $$B=O$$ {% endraw %} 

	- Diagonal Matrix {% raw %} $$diag(1,2,-5,3)=\left[\begin{matrix} 1&0&0&0\\0&2&0&0\\0&0&-5&0\\0&0&0&3\end{matrix}\right] \\
		I_3=\left[\begin{matrix}1&0&0\\0&1&0\\0&0&1\end{matrix}\right]$$ {% endraw %} 

	- Multiplication of Identity Matrix {% raw %} $$ AI_n = I_nA = A,\ \forall A \in \R^{n\times n} $$ {% endraw %} 

	- Transpose of Matrix {% raw %} $$b_{ji}=a_{ij}$$ {% endraw %}  {% raw %} $$(A+B)^T=A^T+B^T\\
		(\alpha A)^T=\alpha A^T\\
		(A^T)^T=A\\
		(AB)^T=B^TA^T$$ {% endraw %} 

	- Symmetric Matrix {% raw %} $$A=A^T$$ {% endraw %} 

	- Skew-symmetric Matrix {% raw %} $$A^T=-A$$ {% endraw %} 

	- Any square matrix can be written as a sum of a symmetric matrix and a skew-symmetric matrix. {% raw %} $$A=\frac{A+A^T}{2}+\frac{A-A^T}{2}$$ {% endraw %} 

	- Invertible Matrixalso called nonsingular or nondegenerate matrices
		Matrix inverse isunique {% raw %} $$AB=BA=I_n\\
		B=A^{-1}\\
		\boldsymbol{x}=A^{-1}\boldsymbol{b}$$ {% endraw %} 

		- Matrix Inverse of a Matrix Transpose {% raw %} $$(A^T)^{-1}=(A^{-1})^T$$ {% endraw %} 

		- Matrix Inverse of a Scalar Multiple {% raw %} $$(\alpha A)^{-1}=\frac{1}{\alpha}A^{-1}$$ {% endraw %} 

		- Matrix Inverse of a Matrix Inverse {% raw %} $$(A^{-1})^{-1}=A$$ {% endraw %} 

		- Matrix Inverse of Matrices Product {% raw %} $$(AB)^{-1}=B^{-1}A^{-1}$$ {% endraw %} 

- Slide 6 - Matrix partition and elementary matrix Slide06.pdf

	- ![](https://notes.bilibiili.com/v2/data/images/505322/5c8e2341d096cfcc17cf9449a43836a5.png)

	- ![](https://notes.bilibiili.com/v2/data/images/505322/0b49ceaeb36b4d719c771c6b23a3b654.png)

	- Elementary MatricesPerformexactly onetype of elementary row operations to an identity matrix. 只能一次操作

		- Type I: 行交换 Row Exchange Matrix

		- Type II: 行标量乘

		- Type III: 行+标量乘行

		- For a given matrix {% raw %} $$A$$ {% endraw %} , performing elementary row operation for {% raw %} $$A$$ {% endraw %}  is equivalent topremultiplying {% raw %} $$A$$ {% endraw %}  by the corresponding elementary matrix.

		- Elementary Matrices are Invertible and Their Inverse are also Elementary Matrices {% raw %} $$E_{R_iR_j}^{-1}=E_{R_iR_j}\\
			E^{-1}_{\alpha R_i}=E_{\frac{1}{\alpha}R_i}\ (\alpha \neq 0)\\
			E^{-1}_{\beta R_i+R_j} = E_{-\beta R_i+R_j}$$ {% endraw %} 

	- Permutation matrix
		Exactly one entry of {% raw %} $$1$$ {% endraw %}  in each row and each colomn and {% raw %} $$0$$ {% endraw %} s elsewhere.
		不一定是 Elementary Matrix

- Slide 7 - LU decomposition Slide07.pdf

	- upper triangular matrix {% raw %} $$a_{ij}=0\ for\ i\gt j$$ {% endraw %} 
		unit ~: 对角线为1

	- Lower triangular matrix {% raw %} $$a_{ij}=0\ for\ i\lt j$$ {% endraw %} 
		unit ~: 对角线为1

	-  {% raw %} $$With\ the\ same\ size\\
		Upper\times Upper=Upper\\
		Lower\times Lower=Lower$$ {% endraw %} 

	- LU-decomposition {% raw %} $$[A|I] \to [U|B]\\
		[B|I] \to [I|L]\\
		A=LU$$ {% endraw %} 

	- LDU decomposition for a nonsingular matrix

- Slide 8 - Find Matrix Inverse by using elementary martices/row operations Slide08.pdf

	- Row Equivalent Matrices 可以 Row Operation 得到（乘 Elementary Matrix）

	- Theorem 8.2 (Equivalent conditions for invertible matrix) {% raw %} $$A \in \R^{n \times n}$$ {% endraw %} , the following are equivalent:

		-  {% raw %} $$A$$ {% endraw %}  is invertible

		- The linear system {% raw %} $$Ax=0$$ {% endraw %}  has only a trivial solution

		- Matrix {% raw %} $$A$$ {% endraw %}  is row equivalent to {% raw %} $$I _n
			$$ {% endraw %} 

		-  {% raw %} $$A$$ {% endraw %}  is a product of elementary matrices

		- There exists an invertible matrix {% raw %} $$E \in \R^{n \times n}$$ {% endraw %}  such that {% raw %} $$EA=I_n$$ {% endraw %} 

		-  {% raw %} $$Ax=b$$ {% endraw %}  has a unique solution for any {% raw %} $$b$$ {% endraw %} 

		-  {% raw %} $$det(A) \neq 0$$ {% endraw %} 

	- Method to find {% raw %} $$A^{-1}$$ {% endraw %}  {% raw %} $$[A|I] \xrightarrow{Gauss\ Jordan\ Elimination}[I|P], then\ P=A^{-1}$$ {% endraw %} 

	- One-Sided Inverse Verification is Sufficient

	- The product {% raw %} $$AB$$ {% endraw %}  is nonsingular if and only if {% raw %} $$A$$ {% endraw %}  and {% raw %} $$B$$ {% endraw %}  are both nonsingular.

- Slide 9 - Vectors I Slide09.pdf

	- (Particular Solution and Homogeneous Solution)If {% raw %} $$w_0$$ {% endraw %}  is one solution of the linear system {% raw %} $$A\boldsymbol{x}=\boldsymbol{b}$$ {% endraw %} , then we can treat {% raw %} $$w_0$$ {% endraw %}  as aparticular solutionfor {% raw %} $$A\boldsymbol{x}=\boldsymbol{b}$$ {% endraw %} ,  The solution(s) of {% raw %} $$A\boldsymbol{x}=\boldsymbol{0}$$ {% endraw %}  are called thehomogeneous solutionsfor the corresponding linear system {% raw %} $$A\boldsymbol{x}=\boldsymbol{b}$$ {% endraw %} .

- Slide 10 - Vectors II

	- Linearly independent 每一个系数都是0
		Linearly dependent 只要有一个系数不是0
		
		If S is a linearly dependent set, then each vector in S is a linear combination of other vectors in S.False 比如combination时系数为0的不能被表示 但一定有一些是可以被其他的 linear combination 表示的
		
		Any set containing the zero vector is linearly dependent.True 因为0前的系数可以是任何，满足有一个系数不是0
		
		Any nonempty subset of a linearly independent set of vectors {% raw %} $$\{v_1, . . . , v_n\}$$ {% endraw %}  is also linearly independent.True 如果有 subset 是 Linearly Dependent 的，那么保持这些系数不变，其他系数为 0，可以得到扩展后的集是 Linearly Dependent 的，所以所有的 subset 是 independent 的.
		
		Any nonempty subset of a linearly dependent set of vectors {% raw %} $$\{v_1, . . . , v_n\}$$ {% endraw %}  is also linearly dependent.False 比如一个 independent 加个 0 就是 dependent

- Slide 11 - Vectors Spaces Slide11.pdf

	- Vector Space {% raw %} $$\R\ \R^n\ \R^{m\times n}\ P_n\ C[a,b]$$ {% endraw %} Additive Closure
		Scalar Closure
		
		8 Axiom
		加法
		A1 交换率
		A2 结合率
		A3 {% raw %} $$\boldsymbol{u} + \boldsymbol{0} = \boldsymbol{u}$$ {% endraw %} 
		乘法
		A4 {% raw %} $$-\boldsymbol{u}=(-1)\boldsymbol{u}$$ {% endraw %} A5 A6 向量部分可分配 标量部分可分配
		A7 标量部分可结合
		A8 标量 {% raw %} $$\times 1$$ {% endraw %}  不变

	- Subspace {% raw %} $$\boldsymbol{0} \in H $$ {% endraw %} 
		Additive Closure
		Scalar Closure

	- ![](https://notes.bilibiili.com/v2/data/images/505322/f0f6d4d14f34a711e95f699d2a65d02c.png)

	- Linearly independent 每一个系数都是0
		Linearly dependent 只要有一个系数不是0

		- If S is a linearly dependent set, then each vector in S is a linear combination of other vectors in S.False 比如combination时系数为0的不能被表示 但一定有一些是可以被其他的 linear combination 表示的

		- Any set containing the zero vector is linearly dependent.True 因为0前的系数可以是任何，满足有一个系数不是0

		- Any nonempty subset of a linearly independent set of vectors {% raw %} $$\{v_1, . . . , v_n\}$$ {% endraw %}  is also linearly independent.True 如果有 subset 是 Linearly Dependent 的，那么保持这些系数不变，其他系数为 0，可以得到扩展后的集是 Linearly Dependent 的，所以所有的 subset 是 independent 的.

		- Any nonempty subset of a linearly dependent set of vectors {% raw %} $$\{v_1, . . . , v_n\}$$ {% endraw %}  is also linearly dependent.False 比如一个 independent 加个 0 就是 dependent

	- Spanning Set 是创建一个 Span 空间的来源向量
		A spans B 是说 {% raw %} $$B=Span(A)$$ {% endraw %}  A 是 B 的 Spanning Set
		B 是 A 的 Span

- Slide 12 - Basis and dimension Slide12.pdf

	- Basis {% raw %} $$u=\{\vec{u_1},\cdots,\vec{u_m}\}\\
		u\ is\ linearly\ independent\\
		Span(u)=V\\
		So\ u\ is\ a\ basis\ of\ V$$ {% endraw %} 

	- A vector set is linearly dependent if number of vectors in the set is larger than number of vectors in the basis

	- 维数够而且还线性独立就可以推出是基底

	- Transition Matrix between two bases
		想把用 u 表示的转为用 v，就用 v 来表示 u 的每一个向量 {% raw %} $$[x]_v=A[x]_u\ where\ the\ jth\ column\ of\ A\ is\ [u_j]_v\\
		[x]_u=A^{-1}[x]_v\ where\ the\ jth\ column\ of\ A\ is\ [u_j]_v$$ {% endraw %} 

- Slide 13 - Null Space, Column Space Slide13.pdf

	- The Null Space of {% raw %} $$A$$ {% endraw %}  is the solution set for {% raw %} $$A\vec{x}=\vec{0}$$ {% endraw %} .

	- Span Colomn 就得到 Column Space

	- 找 Column Space 的方法是先求 Reduced Row Echelon Form，然后把 Pivot Column 对应的原矩阵列作为基底

		- 行变换会改变列空间

	- 找 Null Space 是用 Non-pivot column 对应的变量（free variable）表示其他的

	-  {% raw %} $$dim(Col (A)) + dim(Null (A)) =n$$ {% endraw %} 

- Slide 14 - Row Space and Rank Slide14.pdf

	-  {% raw %} $$Row (A) = Col (A^T)$$ {% endraw %}  {% raw %} $$Row (A^T) = Col (A)$$ {% endraw %} 
		其实是 行向量的 Transpose

	- Row operations preserves the row space

	-  {% raw %} $$r= dim(Row (A)) = dim(Col (A))$$ {% endraw %}  is called the rank of {% raw %} $$A$$ {% endraw %} , denoted by {% raw %} $$rank(A)$$ {% endraw %} .

	- Full Rank Matrix {% raw %} $$rank(A) = min(m,n)$$ {% endraw %} 

		- Rank-Nullity Theorem {% raw %} $$rank(A) +n(A) =n$$ {% endraw %} 

- Slide 15 - Determinants I Slide15.pdf

	- ![](https://notes.bilibiili.com/v2/data/images/505322/ef824568ba2ffebf756844532da78d40.png)Cofactor 是 -1 的几次方和一个 Minor 的乘积

	- Upper/Lower triangular 的 det 是对角线乘积

	- Determinant of Transpose {% raw %} $$det(A^T)=det(A)$$ {% endraw %} 

	- Determinant with Equal Rows or Columns {% raw %} $$det(A)=0$$ {% endraw %} 

- Slide 16 - Determinants II Slide16.pdf

	- Determinant for Row or Column Interchange {% raw %} $$B$$ {% endraw %}  is obtained by exchange exactly one row or column with another in {% raw %} $$A$$ {% endraw %} . Then we have {% raw %} $$det(B)=-det(A)$$ {% endraw %} 

	- Property 16.3 {% raw %} $$\alpha \neq 0\\A \xrightarrow{R_i\to \alpha R_i}B\ or\ A\xrightarrow{C_i\to \alpha C_i} B\\
		Then\ det(B)=\alpha det(A)$$ {% endraw %} 

	- Lemma 16.4 {% raw %} $$a_{i1}A_{j1}+\cdots+a_{in}A_{jn}
		=\begin{cases} 
		      \text{det}(A) & \text{if}\ i=j,\\
		      0 & \text{if}\ i\neq j.
		   \end{cases}$$ {% endraw %} 

	- Property 16.5 {% raw %} $$A\xrightarrow{R_j\to \beta R_i+R_j} B (i\neq j)\\
		Or\ C_j\to \beta C_i+C_j\\
		Then\ \text{det}(B)=\text{det}(A)$$ {% endraw %} 

	- Property 16.6 (Determinants of Three Elementary Matrices)
		第一类：行交换 {% raw %} $$det=-1$$ {% endraw %} 
		第二类：行标量乘 {% raw %} $$det=\alpha$$ {% endraw %} 
		第三类：一行加另一行标量乘 {% raw %} $$det=1$$ {% endraw %} 

	- Determinants of Matrices Product
		If A and B are square matrices, {% raw %} $$det(AB)=det(A)det(B)$$ {% endraw %} 

	- Property 16.12
		两等大矩阵仅一行或一列不同，行列式相加=那一行或一列相加其他不变的矩阵的行列式

	- Definition 16.13 Adjoint Matrix
		在原矩阵 {% raw %} $$a_{ij}$$ {% endraw %}  位置放 Cofactor {% raw %} $$A_{ij}=(-1)^{i+j}det(M_{ij})$$ {% endraw %}  再 Transpose 就得到了 Adjoint Matrix {% raw %} $$A\ adj(A)=det(A)I_n$$ {% endraw %}  {% raw %} $$If\ det(A)\neq 0, A^{-1}=\frac{1}{det(A)}adj(A)$$ {% endraw %}  -> method to find {% raw %} $$A^{-1}$$ {% endraw %} 

- Slide 17 - Linear Transformation I Slide17.pdf

	- Linear transformation
		线性变换是一种保持向量加法和标量乘法的映射. 这意味着：
		向量的加法在变换前后保持一致.
		标量乘法在变换前后保持一致.

	- Property 17.5 (Property of Linear transformation) {% raw %} $$L(\boldsymbol{0}_V)=\boldsymbol{0}_W\\
		L(\alpha_1\boldsymbol{u}_1+\cdots+\alpha_n\boldsymbol{u}_n)=\alpha L(\boldsymbol{u}_1)+\cdots+\alpha_n L(\boldsymbol{u}_n)\\
		L(-\boldsymbol{u})=-L(\boldsymbol{u})$$ {% endraw %} 

	- Definition 17.8 (Kernel of the Linear transformation)
		Let {% raw %} $$L$$ {% endraw %}  be a linear transformation from {% raw %} $$V$$ {% endraw %}  to {% raw %} $$W$$ {% endraw %} , then the kernel of {% raw %} $$L$$ {% endraw %} , denoted by {% raw %} $$ker(L)$$ {% endraw %}  is defined as {% raw %} $$ker(L) = \{v\in V | L(v) =0_W\}$$ {% endraw %} 
		定义域V中被操作后能变0向量的所有向量集合叫 Kernel

	- Definition 17.9 (Image and Range)
		Let {% raw %} $$L$$ {% endraw %}  be a linear transformation from {% raw %} $$V$$ {% endraw %}  to {% raw %} $$W$$ {% endraw %}  and let {% raw %} $$S$$ {% endraw %}  be a subspace of {% raw %} $$V$$ {% endraw %} , the image of {% raw %} $$S$$ {% endraw %} , denoted by {% raw %} $$L(S)$$ {% endraw %} , is dened by {% raw %} $$L(S) =\{\boldsymbol{w}\in W | \exists \boldsymbol{v}\in S, s.t.\ L(\boldsymbol{v}) = \boldsymbol{w}\}$$ {% endraw %}  The image of the entire vector space {% raw %} $$V$$ {% endraw %} , i.e., {% raw %} $$L(V)$$ {% endraw %}  is called the range of {% raw %} $$L$$ {% endraw %} .
		定义域V，子集S对应值域叫S的Image，整个定义域V对应值域叫L这个运算的range（其实也是V的Image）

	- Theorem 17.10
		Kernel是定义域的子空间，Image是值域的子空间

	- Theorem 17.12 (Matrix Representation for linear transformation between Eulerian vector spaces w.r.t. standard bases) {% raw %} $$L:\R^n \to \R^m$$ {% endraw %}  所对应的矩阵 {% raw %} $$A\in \R^{m\times n}$$ {% endraw %} ，满足列向量 {% raw %} $$\boldsymbol{a}_i=L(\boldsymbol{e}_i)$$ {% endraw %} ， {% raw %} $$\boldsymbol{e}$$ {% endraw %}  是 {% raw %} $$\R^n$$ {% endraw %}  空间的标准基底.

	- Example 17.14
		二维平面起点为原点的逆时针旋转： {% raw %} $$A=\left[\begin{matrix}\cos\theta & -\sin\theta\\ \sin\theta & \cos\theta\end{matrix}\right]\\
		L\left(\left[\begin{matrix}x\\ y\end{matrix}\right]\right)=A\left[\begin{matrix}x\\ y\end{matrix}\right]=\left[\begin{matrix}x\cos\theta-y\sin\theta\\ x\sin\theta+y\cos\theta\end{matrix}\right]$$ {% endraw %} 二维平面起点为原点的顺时针旋转 {% raw %} $$B=\left[\begin{matrix}\cos\theta & \sin\theta\\ -\sin\theta & \cos\theta\end{matrix}\right]\\
		L\left(\left[\begin{matrix}x\\ y\end{matrix}\right]\right)=B\left[\begin{matrix}x\\ y\end{matrix}\right]=\left[\begin{matrix}xcos\theta+ysin\theta\\ -xsin\theta+ycos\theta\end{matrix}\right]$$ {% endraw %} 

- Slide 18 - Linear Transformation II Slide18.pdf

	- Theorem 18.1 (Matrix Representation for General Vector Spaces) {% raw %} $$L: V\to W\,\,[L({\bf u})]_w=A[{\bf u}]_v,\forall{\bf u} \in V$$ {% endraw %} ，转换矩阵 {% raw %} $$A$$ {% endraw %} 每列为 {% raw %} $$V$$ {% endraw %} 基底提前经 {% raw %} $$L$$ {% endraw %} 变换后转到 {% raw %} $$W$$ {% endraw %} 

	- Theorem 18.4 (Similarity Result in general vector space) {% raw %} $$E$$ {% endraw %} 、 {% raw %} $$F$$ {% endraw %} 为空间 {% raw %} $$V$$ {% endraw %} 两基底， {% raw %} $$S$$ {% endraw %} 矩阵可从 {% raw %} $$F$$ {% endraw %} 转 {% raw %} $$E$$ {% endraw %} ， {% raw %} $$A$$ {% endraw %} 是 {% raw %} $$L$$ {% endraw %} 操作在 {% raw %} $$E$$ {% endraw %} 的矩阵表示， {% raw %} $$B$$ {% endraw %} 是 {% raw %} $$L$$ {% endraw %} 操作在 {% raw %} $$F$$ {% endraw %} 的矩阵表示， {% raw %} $$B=S^{-1}AS$$ {% endraw %}  就是先从 {% raw %} $$F$$ {% endraw %} 转 {% raw %} $$E$$ {% endraw %} ，做完 {% raw %} $$L$$ {% endraw %} 操作再转回来（从右往左看）

	- Definition 18.5 (Similar) {% raw %} $$A$$ {% endraw %} 、 {% raw %} $$B$$ {% endraw %} 为方阵， {% raw %} $$A$$ {% endraw %} 、 {% raw %} $$B$$ {% endraw %}  Similar 是说 there exists a nonsingular matrix {% raw %} $$S$$ {% endraw %}  such that {% raw %} $$B=S^{-1}AS$$ {% endraw %} 

- Slide 19 - Orthogonality Slide19.pdf

	- Let x and y are two vectors in {% raw %} $$\R^n$$ {% endraw %} , then the product {% raw %} $$x^Ty$$ {% endraw %}  is called the scalar product 即类似点乘

	- Definition 19.1 (Euclidean Length)
		即模长

	- Definition 19.3 (Distance) {% raw %} $$||x-y||$$ {% endraw %} 

	- Lemma 19.5 (Cauchy-Schwartz Inequality) {% raw %} $$|x^Ty|\le||x||\ ||y||$$ {% endraw %} 

	- Theorem 19.6 (Scalar Product in terms of Vector Length) {% raw %} $$ x^Ty=||x||\ ||y||cos\theta, 0\le \theta \le \pi$$ {% endraw %} 

	- Definition 19.7 (Orthogonal Vectors in {% raw %} $$\R^n$$ {% endraw %} )
		Orthogonal <-> {% raw %} $$x^Ty=0$$ {% endraw %} 

	- Theorem 19.9 (Pythagorean's Law)
		Orthogonal <-> {% raw %} $$||x+y||^2=||x||^2+||y||^2$$ {% endraw %} 

	- Definition 19.11 (Orthogonal Subspaces in {% raw %} $$\R^n$$ {% endraw %} )
		两空间内任选向量都垂直
		可推出交集只有0向量

	- Definition 19.13 (Orthogonal Complement)
		同一空间内与子集 Y 中任一向量都垂直的所有向量集合叫 Y 的 Orthogonal Complement {% raw %} $$Y^{\perp}$$ {% endraw %} 

	- Proposition 19.15 (Proposition of Orthogonal Complements)
		Y 是 {% raw %} $$\R^n$$ {% endraw %}  的 subspace，那么 {% raw %} $$Y^\perp$$ {% endraw %}  也是

	- Theorem 19.17 (Fundamental Subspaces Theorem) {% raw %} $$Null(A)=Col(A^T)^\perp=Row(A)^\perp$$ {% endraw %}  {% raw %} $$Null(A^T)=Col(A)^\perp=Row(A^T)^\perp$$ {% endraw %} 

	- Theorem 19.19
		If {% raw %} $$S$$ {% endraw %}  is a subspace of {% raw %} $$\R^n$$ {% endraw %} , then {% raw %} $$dim\ S+dim\ S^\perp = n$$ {% endraw %} 
		and
		a basis for {% raw %} $$S$$ {% endraw %}  combined with a basis for {% raw %} $$S^\perp$$ {% endraw %}  is a basic for {% raw %} $$\R^n$$ {% endraw %} 

* If {% raw %} $$S$$ {% endraw %}  is a subspace of {% raw %} $$\R^n$$ {% endraw %} , {% raw %} $$(S^\perp)^\perp=S$$ {% endraw %} 

- Slide 20 - Orthogonality II Slide20.pdf

	- Definition 20.1 (Direct sum)
		U V 是 W 的 subspace，任一 {% raw %} $$\mathbf{w}\in W$$ {% endraw %}  可被写成唯一的 {% raw %} $$\mathbf{u}+\mathbf{v}$$ {% endraw %} ，W 是空间 U 和 V 的 Direct Sum {% raw %} $$W=U \oplus V$$ {% endraw %} 

	- Theorem 20.2 (Direct sum of {% raw %} $$\R^n$$ {% endraw %} ) {% raw %} $$S$$ {% endraw %}  subspace of {% raw %} $$\R^n$$ {% endraw %} , {% raw %} $$\R^n = S \oplus S^\perp$$ {% endraw %} 

	- Definition 20.5 (Residual) 残差
		对于一个线性系统 {% raw %} $$A\mathbf{x}=\mathbf{b}(A\in \R^{m\times n},\mathbf{b}\in\R^m)$$ {% endraw %} 中的每个 {% raw %} $$\mathbf{x}\in\R^n$$ {% endraw %} ， {% raw %} $$r(\mathbf{x})=\mathbf{b}-A\mathbf{x}$$ {% endraw %} 

	- Definition 20.6 (Least square solution)
		是 {% raw %} $$\hat{\mathbf{x}}$$ {% endraw %} ，使残差最小. 即 {% raw %} $$||r(\hat{\mathbf{x}})||=min||r(\mathbf{x})||$$ {% endraw %} 

	- Theorem 20.7 (Projection onto a Subspace) 投影
		对于任意向量 {% raw %} $$\mathbf{b}\in\R^m$$ {% endraw %} ，我们能找到子空间 {% raw %} $$S$$ {% endraw %}  内某向量 {% raw %} $$\mathbf{p}$$ {% endraw %}  使得 {% raw %} $$1.\ \mathbf{b}-\mathbf{p}\in S^\perp$$ {% endraw %}  即连线是空间垂直补集（三维即连线垂直于面） {% raw %} $$2.\ ||\mathbf{b}-\mathbf{y}||\ge||\mathbf{b}-\mathbf{p}||,\forall \mathbf{y}\in S$$ {% endraw %} （原向量与投影共始点时末点连线长度最短）

		- ![](https://notes.bilibiili.com/v2/data/images/505322/cc86b71f06e4653fca2016320d463ca2.png)

		- If {% raw %} $$\mathbf{b}\in S$$ {% endraw %} , then the projection of {% raw %} $$\mathbf{b}$$ {% endraw %}  onto {% raw %} $$S$$ {% endraw %}  is just {% raw %} $$\mathbf{b}$$ {% endraw %} .

		- Remark 2 对于一个线性系统 {% raw %} $$A\mathbf{x}=\mathbf{b}(A\in \R^{m\times n},\mathbf{b}\in\R^m)$$ {% endraw %} ，当 {% raw %} $$\mathbf{x}=\hat{\mathbf{x}}(A\hat{\mathbf{x}}\text{ is the projection of }\mathbf{b}\text{ onto Col(A)})$$ {% endraw %} ，残差最小.
			同时还有（ {% raw %} $$\forall \mathbf{y}\in Col(A)$$ {% endraw %} ） {% raw %} $$\mathbf{b}-A\hat{\mathbf{x}}\perp A\hat{\mathbf{x}}-A\mathbf{y}\in Col(A)\\
			||\mathbf{b}-A\mathbf{y}||^2\ge||\mathbf{b}-A\hat{\mathbf{x}}||^2$$ {% endraw %} 

	- Theorem 20.8 (Normal equations for the linear system) 对于一个线性系统 {% raw %} $$A\mathbf{x}=\mathbf{b}(A\in \R^{m\times n},\mathbf{b}\in\R^m)$$ {% endraw %} ， {% raw %} $$\mathbf{b}$$ {% endraw %}  在 {% raw %} $$Col(A)$$ {% endraw %}  的投影是 {% raw %} $$\mathbf{p}$$ {% endraw %} ， {% raw %} $$\mathbf{p}=A\hat{\mathbf{x}}\in Col(A)\\
		\mathbf{b}-A\hat{\mathbf{x}} \in Col(A)^\perp=Null(A^T)\\
		||\mathbf{b}-A\mathbf{x}||\ge||\mathbf{b}-A\hat{\mathbf{x}}||$$ {% endraw %}  for any {% raw %} $$\mathbf{x}\in\R^n$$ {% endraw %}  {% raw %} $$A^T(\mathbf{b}-A\hat{\mathbf{x}})=0\\
		A^TA\hat{\mathbf{x}}=A^T\mathbf{b}$$ {% endraw %} (Normal Equation)

	- Theorem 20.9 (Unique Solution Condition for the Normal Equations)
		当 {% raw %} $$A\in \R^{m\times n}$$ {% endraw %}  有 rank n，normal equations 有唯一解 {% raw %} $$\hat{\mathbf{x}}=(A^TA)^{-1}A^T\mathbf{b}$$ {% endraw %} 

		- Projection matrix: {% raw %} $$\mathbf{p}=A\hat{\mathbf{x}}=A(A^TA)^{-1}A^T\mathbf{b}=P\mathbf{b}\\
			P=A(A^TA)^{-1}A^T$$ {% endraw %} 

	- Definition 20.10 (Idempotent) {% raw %} $$A=A^2$$ {% endraw %} Projection matrix is an idempotent matrix.

	- interpolation polynomial 插值多项式

	- Runge's phenomenon 多 oscillations 的数据比较难找到((interpolation polynomial 插值多项式))

	- ![](https://notes.bilibiili.com/v2/data/images/505322/1b17a1d8ae0bd2e23e115b65686e349e.png)

- Slide 21 - Orthogonality III Slide21.pdf

	- Definition 21.1 (Inner Product Space over Real Number Field)
		若有 Inner Product Operation 在 {% raw %} $$V$$ {% endraw %} ，那么 {% raw %} $$V$$ {% endraw %}  是 Inner Product Space

		- Inner Product

			- Definition:

				- Assign a real number {% raw %} $$\left&lt; \mathbf{x},\mathbf{y}\right&gt; $$ {% endraw %}  for each pair of vectors {% raw %} $$\mathbf{x},\mathbf{y}\in V$$ {% endraw %} 

				-  {% raw %} $$\left&lt; \mathbf{x},\mathbf{x}\right&gt;  \ge 0$$ {% endraw %}  when {% raw %} $$\mathbf{x}=0$$ {% endraw %}  equal

				-  {% raw %} $$\left&lt; \mathbf{x},\mathbf{y}\right&gt; =\left&lt; \mathbf{y},\mathbf{x}\right&gt; ,\forall \mathbf{x},\mathbf{y}\in V$$ {% endraw %} 

				-  {% raw %} $$\left&lt; 
					\alpha\mathbf{x}+\beta\mathbf{y},\mathbf{z}\right&gt; =\alpha\left&lt; \mathbf{x},\mathbf{z}\right&gt; +\beta\left&lt; \mathbf{y},\mathbf{z}\right&gt; ,\forall \mathbf{x},\mathbf{y},\mathbf{z}\in V\text{ and }\alpha,\beta\in \R$$ {% endraw %} 

			- Frobenius inner product: Inner Product defined on {% raw %} $$\R^{m\times n }$$ {% endraw %}  {% raw %} $$\left&lt;A,B\right&gt;=\sum\limits^{m}_{i=1}\sum\limits^{n}_{j=1}a_{ij}b_{ij}$$ {% endraw %} 

				-  {% raw %} $$\left&lt; A,A\right&gt; =\sum\limits^{m}_{i=1}\sum\limits^{n}_{j=1}{a_{ij}}^2\ge 0$$ {% endraw %} , {% raw %} $$a_{ij}=0$$ {% endraw %}  取等

				-  {% raw %} $$\left&lt; A,B\right&gt; =\left&lt; B,A\right&gt; $$ {% endraw %} 

				-  {% raw %} $$\left&lt; 
					\alpha A+\beta B,C\right&gt; =\alpha\left&lt; A,C\right&gt; +\beta\left&lt; B,C\right&gt; $$ {% endraw %} 

			- Inner Product on the vector space {% raw %} $$C[a,b]$$ {% endraw %} : {% raw %} $$\left&lt; f,g\right&gt; =\int_a^b{f(x)g(x)dx}$$ {% endraw %} 

				-  {% raw %} $$\left&lt; f,f\right&gt; =\int_a^bf^2(x)dx\ge0$$ {% endraw %} , {% raw %} $$f(x)\equiv 0$$ {% endraw %}  取等

				-  {% raw %} $$\left&lt; f,g\right&gt; =\left&lt; g,f\right&gt; $$ {% endraw %} 

				-  {% raw %} $$\left&lt; 
					\alpha f+\beta g,h\right&gt; =\alpha\left&lt; f,g\right&gt; +\beta\left&lt; g,h\right&gt; $$ {% endraw %} 

	- Definition 21.2 (Length of the vector in inner product space) {% raw %} $$||\mathbf{v}||=\sqrt{\left&lt; \mathbf{v},\mathbf{v}\right&gt; }$$ {% endraw %} 

		-  {% raw %} $$\forall \mathbf{x}\in \R^n,\ ||x|| = \sqrt{\mathbf{x}^T\mathbf{x}}$$ {% endraw %} 

		-  {% raw %} $$\forall f(x)\in C[a,b], ||f||={(\int_a^bf^2(x)dx)}^{\frac{1}{2}}$$ {% endraw %} 

		-  {% raw %} $$\forall A\in\R^{m\times n},||A||={(\sum\limits^m_{i=1}\sum\limits^n_{j=1}{a_{ij}}^2)}^{\frac{1}{2}}$$ {% endraw %} 

	- Definition 21.3 (Orthogonal in the Inner Product Space) {% raw %} $$\left&lt; \mathbf{x},\mathbf{y}\right&gt; =0$$ {% endraw %} 

	- Theorem 21.4 (Pythagorean's Law for inner product space) {% raw %} $$||\mathbf{u}+\mathbf{v}||^2=||\mathbf{u}-\mathbf{v}||^2=||\mathbf{u}||^2+||\mathbf{v}||^2$$ {% endraw %} 

	- Theorem 21.5 (Cauchy-Schwartz Inequality) {% raw %} $$|\left&lt; \mathbf{u},\mathbf{v}\right&gt; |\le||\mathbf{u}||\ ||\mathbf{v}||$$ {% endraw %} 

	- Definition 21.6 (Normed Vector Space)

		- Norm: A real number {% raw %} $$||\mathbf{v}||\in \R$$ {% endraw %}  is the norm of {% raw %} $$\mathbf{v}$$ {% endraw %} 

		-  {% raw %} $$||\mathbf{v}||\ge 0$$ {% endraw %}  with equality if and only if {% raw %} $$\mathbf{v}=0$$ {% endraw %} 

		-  {% raw %} $$||\alpha\mathbf{v}||=|\alpha|\ ||\mathbf{v}||$$ {% endraw %}  for any scalar {% raw %} $$\alpha$$ {% endraw %} 

		-  {% raw %} $$||\mathbf{u}+\mathbf{v}||\le||\mathbf{u}||+||\mathbf{v}||$$ {% endraw %}  for any {% raw %} $$\mathbf{u},\mathbf{v}\in V$$ {% endraw %} 

	- Theorem 21.7 (Norm on the Inner Product Space) {% raw %} $$||\mathbf{v}||=\sqrt{\left&lt; \mathbf{v},\mathbf{v}\right&gt; }$$ {% endraw %}  defines a norm on {% raw %} $$V$$ {% endraw %} 

	- Definition 21.8 (Orthogonal Set) 一个集合，任取两不同向量 Inner Product 都为 0，那么集合叫 Orthogonal Set

	- Definition 21.9 (Orthonormal Set) 一个由 Unit Vectors 组成的 Orthogonal Set，Unit Vector 是说这个 Vector 的 norm 是 1

	- Theorem 21.10 (Orthogonal set are linearly independent)

		- Remark 1: Linear Independent 不能推出 Orthogonal

		- Remark 2: The set {% raw %} $$\{\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_n\}$$ {% endraw %}  is orthonormal if and only if {% raw %} $$\delta_{ij}
			=\left&lt; \mathbf{v}_i,\mathbf{v}_j\right&gt; 
			=\begin{cases} 
			      1 & \text{if}\ i=j,\\
			      0 & \text{if}\ i\neq j.
			   \end{cases}$$ {% endraw %} 

		- Remark 3: Orthogonal 转 Orthonormal (Normalization) {% raw %} $$\{\mathbf{u}_1, \mathbf{u}_2, \cdots, \mathbf{u}_n\} \Rightarrow \{\frac{\mathbf{u}_1}{||\mathbf{u}_1||}, \frac{\mathbf{u}_2}{||\mathbf{u}_2||}, \cdots, \frac{\mathbf{u}_n}{||\mathbf{u}_n||}\} $$ {% endraw %} 

	- Definition 21.12 (Orthonormal Basis) {% raw %} $$\{\mathbf{u}_1,\cdots,\mathbf{u}_m\}$$ {% endraw %} 是 orthonormal 的且 {% raw %} $$V=Span(\mathbf{u}_1,\cdots,\mathbf{u}_m)$$ {% endraw %} 

	- Theorem 21.13 (Coordinate w.r.t orthonormal basis)
		Inner Product Vector Space {% raw %} $$V$$ {% endraw %}  有 Orthonormal Basis {% raw %} $$\{\mathbf{u}_1,\cdots,\mathbf{u}_m\}$$ {% endraw %} ， {% raw %} $$\mathbf{v}\in V$$ {% endraw %}  满足 {% raw %} $$\mathbf{v}=\sum\limits_{i=1}^m\left&lt; \mathbf{v},\mathbf{u}_i
		\right&gt; \mathbf{u}_i$$ {% endraw %} 

		- Inner Product Vector Space {% raw %} $$V$$ {% endraw %}  有 Orthogonal Basis {% raw %} $$\{\mathbf{u}_1,\cdots,\mathbf{u}_m\}$$ {% endraw %} ，则 {% raw %} $$\mathbf{v}\in V$$ {% endraw %}  满足 {% raw %} $$\mathbf{v}=\sum\limits_{i=1}^m\frac{\left&lt; \mathbf{v},\mathbf{u}_i\right&gt; }{||\mathbf{u}_i||^2}\mathbf{u}_i$$ {% endraw %} 

	- Property 21.18(Properties for orthogonal matrix)

		- Definition 21.15 (Orthogonal Matrix) {% raw %} $$Q\in \R^{n\times n}$$ {% endraw %} ，若 {% raw %} $$Q$$ {% endraw %}  的列向量是 {% raw %} $$\R^n$$ {% endraw %}  中的一个OrthonormalSet，则 {% raw %} $$Q$$ {% endraw %}  是OrthogonalMatrix

		- Theorem 21.16 (Equivalent Condition for Orthogonal Matrix)
			Orthogonal Matrix IFF {% raw %} $$Q^{-1}=Q^T$$ {% endraw %} 

		-  {% raw %} $$Q^TQ=I_n$$ {% endraw %} 

		-  {% raw %} $$\left&lt; Q\mathbf{x},Q\mathbf{y}\right&gt; =\left&lt; \mathbf{x},\mathbf{y}\right&gt; \\
			\left&lt; Q\mathbf{x},Q\mathbf{y}\right&gt; =(Q\mathbf{y})^TQ\mathbf{x}=\mathbf{y}^TQ^TQ\mathbf{x}=\mathbf{y}^T\mathbf{x}=\left&lt; \mathbf{x},\mathbf{y}\right&gt; $$ {% endraw %} 

		-  {% raw %} $$||Q\mathbf{x}||=||\mathbf{x}||$$ {% endraw %} 

	- Theorem 21.5 (Cauchy-Schwartz Inequality)
		If {% raw %} $$\mathbf{u},\mathbf{v}\in V$$ {% endraw %}  ( {% raw %} $$V$$ {% endraw %}  Inner Product Space), we have {% raw %} $$|\left&lt; \mathbf{u},\mathbf{v}\right&gt; |\le||\mathbf{u}||\ ||\mathbf{v}||$$ {% endraw %} 

- Slide 22 - Orthogonality IV Slide22.pdf

	- Lemma 22.1 (Projection onto a subspace)
		Inner Product Space {% raw %} $$V$$ {% endraw %}  内有 {% raw %} $$\{\mathbf{u}_1,\cdots,\mathbf{u}_m\}$$ {% endraw %}  有 是 Subspace {% raw %} $$S$$ {% endraw %}  的 Orthonormal Basis，对于任意 {% raw %} $$\mathbf{x}\in V$$ {% endraw %} ，我们可以把 {% raw %} $$\mathbf{x}$$ {% endraw %}  投影到 {% raw %} $$S$$ {% endraw %}  (即使 {% raw %} $$\mathbf{x}-\mathbf{p}\perp S$$ {% endraw %} )，那么我们有 {% raw %} $$\mathbf{p}=\sum\limits^m_{i=1}\left&lt; \mathbf{x},\mathbf{u}_i\right&gt; \mathbf{u}_i$$ {% endraw %} 

		- 参考((Theorem 21.13 (Coordinate w.r.t orthonormal basis)
			Inner Product Vector Space {% raw %} $$V$$ {% endraw %}  有 Orthonormal Basis {% raw %} $$\{\mathbf{u}_1,\cdots,\mathbf{u}_m\}$$ {% endraw %} ， {% raw %} $$\mathbf{v}\in V$$ {% endraw %}  满足 {% raw %} $$\mathbf{v}=\sum\limits_{i=1}^m\left&lt; \mathbf{v},\mathbf{u}_i
			\right&gt; \mathbf{u}_i$$ {% endraw %} ))，若是在这个空间内的向量用这个式子表示就可以直接表示。若是本来不在这个空间的向量，用这个式子表示就是投影。这是因为 {% raw %} $$\left&lt; \mathbf{x}-\mathbf{p},\mathbf{u}_i\right&gt; =0$$ {% endraw %} 

	- Gram-Schmidt Process 格拉姆-施密特正交化

		- 使得一个 linearly independent 的向量集变为一个 orthonormal set 的同时维持 Span 不变

		- ![](https://notes.bilibiili.com/v2/data/images/505322/df9c9601b173f57d08db79f427beb54b.png)二维空间图示

		- ![](https://notes.bilibiili.com/v2/data/images/505322/2f91b46fbc091ef0f7ac026f0f5b7010.png)三维空间图示

		- 步骤：

			-  {% raw %} $$\mathbf{v}_1=\frac{\mathbf{u}_1}{||\mathbf{u}_1||}$$ {% endraw %} 

			- Project {% raw %} $$\mathbf{u}_2$$ {% endraw %}  onto {% raw %} $$Span(\mathbf{v}_1)$$ {% endraw %} ， {% raw %} $$\mathbf{p}_1=\left&lt; \mathbf{u}_2,\mathbf{v}_1\right&gt; \mathbf{v}_1$$ {% endraw %} ， {% raw %} $$\mathbf{v}_2=\frac{\mathbf{u}_2-\mathbf{p}_1}{||\mathbf{u}_2-\mathbf{p}_1||}\perp Span(\mathbf{v}_1)$$ {% endraw %} *Now {% raw %} $$Span(\mathbf{v}_1,\mathbf{v}_2)=Span(\mathbf{u}_1,\mathbf{u}_2)$$ {% endraw %} 

			- ...

			- Project {% raw %} $$\mathbf{u}_m$$ {% endraw %}  onto {% raw %} $$Span(\mathbf{v}_1,\cdots,\mathbf{v}_{m-1})$$ {% endraw %} ， {% raw %} $$\mathbf{p}_{m-1}=\left&lt; \mathbf{u}_m,\mathbf{v}_1\right&gt; \mathbf{v}_1+\left&lt; \mathbf{u}_m,\mathbf{v}_2\right&gt; \mathbf{v}_2+\cdots+\left&lt; \mathbf{u}_m,\mathbf{v}_{m-1}\right&gt; \mathbf{v}_{m-1}$$ {% endraw %}  {% raw %} $$\mathbf{v}_m=\frac{\mathbf{u}_m-\mathbf{p}_{m-1}}{||\mathbf{u}_m-\mathbf{p}_{m-1}||}\perp Span(\mathbf{v}_1,\cdots,\mathbf{v}_{m-1})$$ {% endraw %} *Now {% raw %} $$Span(\mathbf{u}_1,\cdots,\mathbf{u}_{m})=Span(\mathbf{v}_1,\cdots,\mathbf{v}_{m})$$ {% endraw %} 

	- Theorem 22.5 (QR decomposition) {% raw %} $$A=QR\\ (A\in \R^{m\times n}\ \text{rank}(A)=n\\
		\text{i.e. Column Vectors Linearly Independent}\\
		Q\in \R^{m\times n}\ \text{Orthogonal Column Vectors}\\
		R\in \R^{n\times n}\ \text{Upper Triangular, Diagonal Positive})$$ {% endraw %} Q 是经Gram-Schmidt 得到的，R用下面方法求

		-  {% raw %} $$R=Q^{-1}A=Q^TA$$ {% endraw %} 

		- Theorem: {% raw %} $$A^TA\hat{x}=A^T{\bf b}$$ {% endraw %}  {% raw %} $$R^TR\hat{x}=R^TQ^Tb$$ {% endraw %} Finally: {% raw %} $$R\hat{x}=Q^T\mathbf{b}$$ {% endraw %}  {% raw %} $$\hat{x}=R^{-1}Q^T\mathbf{b}$$ {% endraw %} 

- Slide 23 - Eigenvalues Slide23.pdf

	- Definition 23.1 (Eigenvalue and eigenvectors)
		Scalar {% raw %} $$\lambda$$ {% endraw %} , Nonzero Vector {% raw %} $$\mathbf{x}$$ {% endraw %} , Square Matrix {% raw %} $$A$$ {% endraw %} , {% raw %} $$A\mathbf{x}=\lambda\mathbf{x}$$ {% endraw %}  {% raw %} $$\lambda$$ {% endraw %}  叫作 eigenvalue (characteristic value), {% raw %} $$\mathbf{x}$$ {% endraw %}  叫做 eigenvector (characteristic vector) w.r.t {% raw %} $$\lambda$$ {% endraw %} 

		- 若 {% raw %} $$\mathbf{u}$$ {% endraw %}  是关于 eigenvalue {% raw %} $$\lambda$$ {% endraw %}  的 eigenvector, {% raw %} $$\alpha\mathbf{u}\ (\alpha\neq 0)$$ {% endraw %} 也是

		- 若 {% raw %} $$\lambda$$ {% endraw %}  是 {% raw %} $$A$$ {% endraw %}  的 eigenvalue, 那么 {% raw %} $$\lambda^s$$ {% endraw %}  是 {% raw %} $$A^S$$ {% endraw %}  的 eigenvalue

	- Theorem 23.3 {% raw %} $$A\in \R^{n\times n}(\Complex^{n\times n})\ \lambda\in\R(\Complex)$$ {% endraw %}  以下命题等效

		-  {% raw %} $$\lambda$$ {% endraw %}  是 {% raw %} $$A$$ {% endraw %}  的 eigenvalue

		-  {% raw %} $$(A-\lambda I)\mathbf{x}=\mathbf{0}$$ {% endraw %} has nontrivial solutions. (As {% raw %} $$\lambda\mathbf{x}=\lambda I\mathbf{x}$$ {% endraw %}  and {% raw %} $$A\mathbf{x}=\lambda\mathbf{x}$$ {% endraw %} )

		-  {% raw %} $$\text{Null}(A-\lambda I)\neq\{\mathbf{0}\}$$ {% endraw %} where {% raw %} $$\text{Null}(A-\lambda I)$$ {% endraw %} is called the eigenspace corresponding to {% raw %} $$\lambda$$ {% endraw %} , is subspace of {% raw %} $$\R^n(\Complex^n)$$ {% endraw %}  when {% raw %} $$\lambda\in\R(\Complex)$$ {% endraw %} 

		-  {% raw %} $$A-\lambda I$$ {% endraw %}  is singular

		-  {% raw %} $$\text{det}(A-\lambda I)=0$$ {% endraw %} 

			- A method to calculate the eigenvalues

			-  {% raw %} $$\lambda$$ {% endraw %}  同时是 {% raw %} $$A$$ {% endraw %}  和 {% raw %} $$A^T$$ {% endraw %}  的 eigenvalue

	- Definition 23.4 (Characteristic Polynomial) {% raw %} $$p_A(\lambda)=\text{det}(A-\lambda I)$$ {% endraw %} is called the characteristic polynomial of {% raw %} $$A$$ {% endraw %} . {% raw %} $$p_A(\lambda)=0$$ {% endraw %}  is called the characteristic equation of {% raw %} $$A$$ {% endraw %} .

	- Theorem 23.9 (Fundamental theorem in Algebra)
		每个 {% raw %} $$n$$ {% endraw %}  次系数可为复数的多项式有 {% raw %} $$n$$ {% endraw %}  个复数根

		- 每个 {% raw %} $$n\times n$$ {% endraw %}  的矩阵有 {% raw %} $$n$$ {% endraw %}  个 eigenvalues

	- Theorem 23.10 (Product and Sum of Eigenvalues) {% raw %} $$A\in\R^{n\times n}(\Complex^{n\times n})\ \ \lambda_i\ \text{eigenvalues}$$ {% endraw %} then: {% raw %} $$\displaystyle\text{det}(A)=\prod_{i=1}^n \lambda_i$$ {% endraw %}  {% raw %} $$\displaystyle\sum_{i=1}^n a_{ii}=\sum_{i=1}^n\lambda_i=tr(A)$$ {% endraw %} 

		-  {% raw %} $$\displaystyle \text{Trace}(A)= tr(A)= \sum_{i=1}^n a_{ii}$$ {% endraw %} 

		-  {% raw %} $$A\text{ is nonsingular}\iff \text{det}(A)\neq 0\iff\text{all eigenvalues }\lambda_i\neq 0$$ {% endraw %} 

		-  {% raw %} $$A$$ {% endraw %} is nonsingular and {% raw %} $$\lambda$$ {% endraw %}  is the eigenvalue of {% raw %} $$A$$ {% endraw %} , Then {% raw %} $$\iff \lambda^{-1}$$ {% endraw %}  is the eigenvalue of {% raw %} $$A^{-1}$$ {% endraw %} 

	- Theorem 23.12 (Similar Matrices Have the Same Eigenvalues) {% raw %} $$B=S^{-1}AS$$ {% endraw %}  Then {% raw %} $$p_B(\lambda)=p_A(\lambda)$$ {% endraw %} 

- Slide 24 - Diagonalization and Spectral Theorem Slide24.pdf

	- Definition 24.1 (Diagonalizable) {% raw %} $$A\in\R^{n\times n}$$ {% endraw %}  is diagonalizable if {% raw %} $$\exist X(X\text{ nonsingular}),\ X^{-1}AX=D$$ {% endraw %}  {% raw %} $$A=XDX^{-1}$$ {% endraw %} , {% raw %} $$X$$ {% endraw %}  diagonalizes {% raw %} $$A$$ {% endraw %} 

	- Theorem 24.2 (Eigenvectors belonging to distinct eigenvalues are linearly independent) {% raw %} $$\lambda_1, \lambda_2,\cdots,\lambda_k$$ {% endraw %} are distinct eigenvalues of {% raw %} $$n\times n$$ {% endraw %}  matrix {% raw %} $$A$$ {% endraw %} , {% raw %} $$\mathbf{x}_1, \mathbf{x}_2,\cdots,\mathbf{x}_k$$ {% endraw %}  are eigenvectors corresponding to {% raw %} $$\lambda_1, \lambda_2,\cdots,\lambda_k$$ {% endraw %} , the {% raw %} $$\mathbf{x}_1, \mathbf{x}_2,\cdots,\mathbf{x}_k$$ {% endraw %}  are linearly independent

	- Theorem 24.3 (Sufficient and Necessary Condition for Diagonalization)
		A {% raw %} $$n\times n$$ {% endraw %}  matrix {% raw %} $$A$$ {% endraw %}  is diagonalizable iff {% raw %} $$A$$ {% endraw %}  has {% raw %} $$n$$ {% endraw %}  linearly independent eigenvectors

		-  {% raw %} $$A$$ {% endraw %}  diagonalizable, column vectors of the diagonalizing matrix {% raw %} $$X$$ {% endraw %}  are eigenvectors of {% raw %} $$A$$ {% endraw %}  and the diagonal elements of {% raw %} $$D$$ {% endraw %}  are the corresponding eigenvalues

		- Diagonalizing Matrix {% raw %} $$X$$ {% endraw %}  is not unique
			Get a new one by reordering the columns or multiplying nonzero scalars for columns on an existing one

		-  {% raw %} $$A^2=XD^2X^{-1},\ A^k=XD^kX^{-1}$$ {% endraw %} 

	- Theorem 24.7 {% raw %} $$A\in \R^{n\times n}\text{ symmetric, eigenvals }\lambda_1,\cdots,\lambda_n\to$$ {% endraw %}  {% raw %} $$1.\ \lambda_i\in\R\\
		2.\ \lambda_i\ne\lambda_j\to \mathbf{x}_i,\mathbf{x}_j\text{ orthogonal (corresponding eigenvecs)}$$ {% endraw %} 

	- Theorem 24.8 (Spectral Theorem (eigen decomposition theorem) for Real Symmetric Matrix) {% raw %} $$A$$ {% endraw %}  real symmetric, {% raw %} $$\exist Q\text{ orthogonal, }Q^{-1}AQ=Q^TAQ=\bigwedge(\text{diagonal})$$ {% endraw %}  {% raw %} $$Q$$ {% endraw %}  diagonalizes {% raw %} $$A$$ {% endraw %} 
		求 {% raw %} $$\bigwedge$$ {% endraw %}  的方法： {% raw %} $$\left[\begin{matrix}\lambda_1&0&0\\0&\lambda_2&0\\0&0&\lambda_3\end{matrix}\right]$$ {% endraw %} 求 {% raw %} $$Q$$ {% endraw %}  的方法：把 eigenvalue 对应的 eigenvector 作为列向量组成矩阵

		- Eigendecomposition or Eigenvalue Decomposition {% raw %} $$A=Q\bigwedge Q^T=\lambda_1\mathbf{q}_1\mathbf{q}_1^T+\cdots+\lambda_n\mathbf{q}_n\mathbf{q}_n^T$$ {% endraw %} 

- Slide 25 - Quadratic Form Slide25.pdf

	- Definition 25.1 (Quadratic Equation with two unknowns) {% raw %} $$ax^2+2bxy+cy^2+dx+ey+f=0 (*)\to\\
		\left[\begin{matrix}x &y\end{matrix}\right]\left[\begin{matrix}a&b\\b&c\end{matrix}\right]\left[\begin{matrix}x\\y\end{matrix}\right]+\left[\begin{matrix}d&e\end{matrix}\right]\left[\begin{matrix}x\\y\end{matrix}\right]+f=0$$ {% endraw %}  {% raw %} $$\mathbf{x} = \begin{bmatrix} x \\ y \end{bmatrix}, \quad A = \begin{bmatrix} a & b \\ b & c \end{bmatrix}$$ {% endraw %}  {% raw %} $${\bf x}^{T}A{\bf x}+[d,e{]{\bf x}+{f}=0}\to (*)$$ {% endraw %} 
		
		-> {% raw %} $${\bf x}^TA{\bf x}$$ {% endraw %} is the quadratic form with quadratic equation {% raw %} $$(*)$$ {% endraw %} . The graph of {% raw %} $$(*)$$ {% endraw %}  is called theconic section

	- Definition 25.3 (Definite quadratic form and definite matrix) {% raw %} $${\bf x}\in\R^n,\,A\in\R^{n\times n}$$ {% endraw %} symmetric {% raw %} $$f({\bf x})={\bf x}^TA{\bf x}$$ {% endraw %} 

		- The quadratic form {% raw %} $$f({\bf x})$$ {% endraw %}  ispositive definiteif {% raw %} $$f({\bf x})\gt 0$$ {% endraw %}  for any {% raw %} $${\bf x \neq 0}$$ {% endraw %} . {% raw %} $$A$$ {% endraw %}  is calledpositive definite matrix.

		- The quadratic form {% raw %} $$f({\bf x})$$ {% endraw %}  ispositive semidefiniteif {% raw %} $$f({\bf x})\ge 0$$ {% endraw %}  for any {% raw %} $${\bf x \neq 0}$$ {% endraw %} . {% raw %} $$A$$ {% endraw %}  is calledpositive semidefinite matrix.

		- The quadratic form {% raw %} $$f({\bf x})$$ {% endraw %}  isindefinite if {% raw %} $$f({\bf x})$$ {% endraw %}  takes diff signs.

		- The quadratic form {% raw %} $$f({\bf x})$$ {% endraw %}  isnegative definiteif {% raw %} $$f({\bf x})\lt 0$$ {% endraw %}  for any {% raw %} $${\bf x \neq 0}$$ {% endraw %} . {% raw %} $$A$$ {% endraw %}  is callednegative definite matrix.

		- The quadratic form {% raw %} $$f({\bf x})$$ {% endraw %}  isnegative semidefiniteif {% raw %} $$f({\bf x})\le 0$$ {% endraw %}  for any {% raw %} $${\bf x \neq 0}$$ {% endraw %} . {% raw %} $$A$$ {% endraw %}  is callednegative semidefinite matrix.

	- MAT 2040's Last Theorem {% raw %} $$A\in\R^{n\times n}$$ {% endraw %}  the following are eq:

		-  {% raw %} $$A$$ {% endraw %}  is positive definite

		- All eigenvalues of {% raw %} $$A$$ {% endraw %}  are positive

		- The leading principal minors are positive

			- Leading principal minor: {% raw %} $$det(\text{leading principal submatrix})$$ {% endraw %} 

			- Leading Principal Submatrix, 从左上角切小矩阵，例 {% raw %} $$A=\left[\begin{matrix}1&2&3\\4&5&6\\7&8&9\end{matrix}\right]\\
				A_1=\left[\begin{matrix}1\end{matrix}\right]\\
				A_2=\left[\begin{matrix}1&2\\4&5\end{matrix}\right]\\
				A_3=\left[\begin{matrix}1&2&3\\4&5&6\\7&8&9\end{matrix}\right]$$ {% endraw %} 

		-  {% raw %} $$A=C^2$$ {% endraw %}  with {% raw %} $$C\gt 0$$ {% endraw %} 

			- (Square root {% raw %} $$C=A^{1\over 2}=\sqrt{A}$$ {% endraw %} )

		-  {% raw %} $$A=LL^T$$ {% endraw %}  with {% raw %} $$L$$ {% endraw %}  being a lower triangular matrix with {% raw %} $$l_{ii}\gt 0\ (i=1,\cdots,n)$$ {% endraw %}  (Cholesky 分解)

		-  {% raw %} $$A=LDL^T$$ {% endraw %}  {% raw %} $$L$$ {% endraw %}  unit lower triangular {% raw %} $$D\gt0$$ {% endraw %}  diagonal

